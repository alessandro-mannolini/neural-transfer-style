{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'immagini\\\\immagini_originali\\\\Ale.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('immagini\\\\immagini_originali', 'Ale.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico il modello vgg19 pre-addestrato\n",
    "\n",
    "model = models.vgg19(weights = True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "        self.model = models.vgg19(pretrained = True).features[:29]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            \n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "                \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qui carico le immagini di cui ho bisogno\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "image_size = 256\n",
    "\n",
    "loader = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# carico immagine originale\n",
    "path_img_orig = \"C:\\\\Users\\\\aless\\\\Documents\\\\03_git_hub_personale\\\\01_transfer_style\\\\immagini\\\\immagini_originali\\\\Ale.jpeg\"\n",
    "original_img = load_image(path_img_orig)\n",
    "# carico immagine con lo stile richiesto\n",
    "path_img_style = \"C:\\\\Users\\\\aless\\\\Documents\\\\03_git_hub_personale\\\\01_transfer_style\\\\immagini\\\\immagini_stile\\\\sentiero.jfif\"\n",
    "style_img = load_image(path_img_style)\n",
    "\n",
    "\n",
    "model = VGG().to(device).eval()\n",
    "\n",
    "#generated = torch.randn(original_img.shape, device = device, requires_grad = True)\n",
    "generated = original_img.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(453775.8438, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aless\\Documents\\03_git_hub_personale\\01_transfer_style\\codice\\neural_transfer_style.ipynb Cella 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Documents/03_git_hub_personale/01_transfer_style/codice/neural_transfer_style.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m200\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Documents/03_git_hub_personale/01_transfer_style/codice/neural_transfer_style.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mprint\u001b[39m(total_loss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aless/Documents/03_git_hub_personale/01_transfer_style/codice/neural_transfer_style.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     generated\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39m\u001b[39mgenerated.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "total_steps = 1000\n",
    "learning_rate = 0.05\n",
    "alpha = 1\n",
    "beta = 0.001\n",
    "\n",
    "optimizer = optim.Adam([generated], lr = learning_rate)\n",
    "\n",
    "for step in range(total_steps):\n",
    "    generated_features = model(generated)\n",
    "    original_img_features = model(original_img)\n",
    "    style_features = model(style_img)\n",
    "    \n",
    "    style_loss = original_loss = 0\n",
    "    \n",
    "    for gen_feature, orig_feature, style_feature in zip(generated_features, original_img_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        original_loss = torch.mean((gen_feature - orig_feature) **2)\n",
    "        \n",
    "        #compute the Gram Matrix\n",
    "        G = gen_feature.view(channel, height*width).mm(\n",
    "            gen_feature.view(channel, height*width).t()\n",
    "        )\n",
    "        \n",
    "        A = style_feature.view(channel, height*width).mm(\n",
    "            style_feature.view(channel, height*width).t()\n",
    "        )\n",
    "        \n",
    "        style_loss += torch.mean((G-A)**2)\n",
    "        \n",
    "        \n",
    "    total_loss = alpha*original_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(total_loss)\n",
    "        generated.save(\"generated.png\")    # c'Ã¨ qualche problema, domani va ricontrollato!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
